BUGS

  Can't restart with an existing database. Suspect something to do with
  Moose being lazy. Or maybe...

  Above problem may be because we don't always die cleanly. Why is that?
  The fact that the HLS packager worker doesn't die (and is the subject
  of the warning about not being killed) also causes us to run out of
  inotify handles after a few packaging jobs.

  Maybe making PipeFork cleanly stackable - so INT/TERM propagate to all
  children is the way forward.

  Died during live streaming. What happened? Need auto-restart in the
  event of packager failure. Ideally with contiguous timestamps but
  discontunities better than nothing.

  What happens if we have to restart with junk in the model? Currently
  best to delete the model but leave the work directories - which
  implies that, at least, we should be able to recover and continue
  based exclusively on the contents of the work directory.

Make ForkPipe commit infanticide reliably.

Wire up ForkPipe and use it to get async notifications from packager.

What happens when an m3u8 transitions from EVENT -> VOD during playback?

Push fragments/manifests to S3.

Log to file as well as console.

Integrate switcher / router.

Fix up DTS/PTS in router: global clock.

Test RTSP from Wirecast. Simulated / actual network conditions.

More verbs in Emitron::Core
  Stream switching
    Slate / preroll / multiple sources
  Stream lifecycle: L->V
  For ROH:
    Live -> VOD  
    Live -> Prerecord -> VOD

Moosify Harmless.

Make encoders run in (NTP) sync: DTS/PTS/GoP

VOD capture?
  Logging?

Authentication / authorisation
  How do we handle that for incoming RTMP?

SSL for web app.

Make webapp run under Apache.
